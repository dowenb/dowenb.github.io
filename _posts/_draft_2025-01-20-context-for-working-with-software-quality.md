---
layout: post
title: Context for working with software quality
date: 2025-01-20T00:00:00.000Z
draft: true
categories: Quality
---

I've recently been exploring what the differences are, between the role of a Quality Engineer, a role I'm currently inhabiting in my work at Ada Health, and other roles in Software Quality.

One of the realisations I had already, that probably isn't news to anyone who has worked multiple jobs in any role in technology, is that the way different roles actually work, the expectations and reality, is very different from company to company.

Where you work, makes a huge different to the opportunities you have to explore and execute on different parts of your job, different tools from your tool box you get to use.

You may already know this, especially if you work in software and especially if you work in quality, context matters. But what do I mean by context? And what do you mean by context?

In this post I will discuss a few key points that have mattered to me, and setting my context, in different companies and teams. Naturally your experience will be different, it must, your context was different, and it still is! But you might find some of what I say relatable, and I hope you find any of it useful.

## Consultancy vs product company

### Consultancy context

I worked for two years, in a small software consultancy company in the UK. The model was simple, the company had a number of clients, with different contracts for different types of work, depending on their needs. The way quality and testing was handled for this different clients was not uniform, but depending on the project and the contract we had with the clients.

The biggest contextual impact here, is how work, and therefore any changes, were paid for. Find a bug that stops us from meeting the acceptance criteria agreed with the client, that's on us! The bug is likely to get fixed, and probably pretty quick, because it might stop us from getting paid. Run! Go fix that bug now!

Is it a bug in requirements? Maybe a whole missing feature? uhoh! This means negotiate a change request with the client, and getting them to pay for the work. Sometimes I would advocate for our clients, that the missing functionality is something we should have specified and flagged early, and the client had a reasonable expectation for it to exist, sometimes, I had to help explain the bad news to the client. Either way, it definitely impacted how I raised issues, and how I had to relate these issues back to the tickets and specifications, I HAD to, or call it our as potential missing work.

Also, I had to be careful what testing I did, how long I spent doing it, and how the testing was recorded and reported. Some clients wanted test scripts, to have as regression packs that we would hand over. Others, didn't want to pay for testing at all, so I had to talk with project managers, to convince them it was worth our time spending our money for me to do testing, even if it wasn't a billable line item.

I understand this is not the only consultancy model that exists, I have friends who have worked for larger consultancies, where they have been effectively loaned out to companies for entire projects. In some of those projects, one might not be testing work that built by the consultancy, but by the client or even a different consultancy. You might be providing specific, or specialist testing services only. I haven't worked in this context, but I know it's out there.

### Product context

Contrast that very commercial, consultancy context, with the one from working for product companies. 

## In a quality team vs in the development team

## TODO third list item
